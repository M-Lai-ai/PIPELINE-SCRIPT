import os
import re
import json
import sys
import time
import hashlib
import logging
import threading
from pathlib import Path
from datetime import datetime
from collections import defaultdict, deque
from itertools import cycle
from queue import Queue, Empty

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from playwright.sync_api import sync_playwright
from colorama import init, Fore, Style
from tqdm import tqdm
import html2text

from pdf2image import convert_from_path
import pytesseract
import numpy as np
import cv2
from PIL import Image
import pypdf

from rich.console import Console
from rich.logging import RichHandler
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.traceback import install
from dotenv import load_dotenv

# Initialisation des modules Rich pour les tracebacks
install()

# Initialisation Colorama
init(autoreset=True)

# Classe pour un formatteur de logs color√©s
class ColoredFormatter(logging.Formatter):
    """Formatter personnalis√© pour ajouter des couleurs, des symboles et des '#' √† la fin des logs."""

    # D√©finition des couleurs et symboles
    COLORS = {
        'DEBUG': Fore.CYAN,
        'INFO': Fore.GREEN,
        'WARNING': Fore.YELLOW,
        'ERROR': Fore.RED,
        'CRITICAL': Fore.RED + Style.BRIGHT
    }

    SYMBOLS = {
        'INFO': '‚úî',
        'WARNING': '‚ö†',
        'ERROR': '‚úò',
        'CRITICAL': '‚úò'
    }

    def format(self, record):
        color = self.COLORS.get(record.levelname, Fore.WHITE)
        symbol = self.SYMBOLS.get(record.levelname, '')
        # Format initial avec le symbole et le timestamp, suivi de '#'
        header = f"{color}{symbol} {self.formatTime(record, self.datefmt)}#"
        # Niveau de log, suivi de '#'
        level = f"- {record.levelname}#"
        # Message, suivi de '#'
        message = f"- {record.getMessage()}#"
        return f"{header}\n{level}\n{message}"

# Classe pour afficher un indicateur dynamique dans le terminal
class MovingIndicator(threading.Thread):
    """Thread pour afficher un indicateur dynamique se d√©pla√ßant de droite √† gauche avec des '#' √† la fin."""

    def __init__(self, delay=0.1, length=20):
        super().__init__()
        self.delay = delay
        self.length = length
        self.running = False
        self.position = self.length - 1  # Commencer √† droite
        self.direction = -1  # D√©placer vers la gauche

    def run(self):
        self.running = True
        while self.running:
            # Cr√©er la repr√©sentation de l'indicateur
            line = [' '] * self.length
            if 0 <= self.position < self.length:
                line[self.position] = '#'
            indicator = ''.join(line) + '##'  # Ajouter '##' √† la fin pour dynamisme
            # Afficher l'indicateur
            sys.stdout.write(f"\r{indicator}")
            sys.stdout.flush()
            # Mettre √† jour la position
            self.position += self.direction
            if self.position == 0 or self.position == self.length - 1:
                self.direction *= -1  # Changer de direction
            time.sleep(self.delay)

    def stop(self):
        self.running = False
        self.join()
        # Nettoyer la ligne de l'indicateur
        sys.stdout.write('\r' + ' ' * (self.length + 2) + '\r')  # +2 pour les '##'
        sys.stdout.flush()

# Classe pour le Crawling Web
class WebCrawler:
    def __init__(self, start_url, max_depth=2, use_playwright=False, excluded_paths=None, download_extensions=None, language_pattern=None, base_dir=None):
        self.start_url = start_url
        self.max_depth = max_depth
        self.use_playwright = use_playwright
        self.visited_pages = set()
        self.downloaded_files = set()
        self.domain = urlparse(start_url).netloc
        self.excluded_paths = excluded_paths or ['selecteur-de-produits']
        self.downloadable_extensions = download_extensions or {
            'PDF': ['.pdf'],
            'Image': ['.png', '.jpg', '.jpeg', '.gif', '.svg'],
            'Doc': ['.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
        }
        self.language_pattern = language_pattern
        self.base_dir = base_dir or f"crawler_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.create_directories()
        self.setup_logging()
        self.stats = defaultdict(int)
        self.all_downloadable_exts = set(ext for exts in self.downloadable_extensions.values() for ext in exts)
        self.content_type_mapping = {
            'PDF': {
                'application/pdf': '.pdf'
            },
            'Image': {
                'image/jpeg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/svg+xml': '.svg',
            },
            'Doc': {
                'application/msword': '.doc',
                'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',
                'application/vnd.ms-excel': '.xls',
                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': '.xlsx',
                'application/vnd.ms-powerpoint': '.ppt',
                'application/vnd.openxmlformats-officedocument.presentationml.presentation': '.pptx',
            }
        }
        self.session = self.setup_session()
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.body_width = 0
        self.html_converter.ignore_images = True
        self.html_converter.single_line_break = False
        if self.use_playwright:
            self.playwright = sync_playwright().start()
            self.browser = self.playwright.chromium.launch(headless=True)
            self.page = self.browser.new_page()
        self.indicator = MovingIndicator(length=20)

    def setup_session(self):
        """Configure une session requests avec retry et timeouts"""
        session = requests.Session()
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.verify = False  # Pour d√©sactiver la v√©rification SSL si n√©cessaire
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                          'AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/91.0.4472.124 Safari/537.36'
        })
        return session

    def create_directories(self):
        """Cr√©e la structure de dossiers n√©cessaire pour le crawler"""
        directories = ['content', 'PDF', 'Image', 'Doc', 'logs']
        for dir_name in directories:
            path = os.path.join(self.base_dir, dir_name)
            os.makedirs(path, exist_ok=True)

    def setup_logging(self):
        """Configure le syst√®me de logging avec couleurs, symboles et '#' √† la fin des lignes"""
        log_format = '%(asctime)s - %(levelname)s - %(message)s'
        formatter = ColoredFormatter(log_format)

        logger = logging.getLogger()
        logger.setLevel(logging.INFO)

        # Handler pour le fichier log (sans couleurs)
        file_handler = logging.FileHandler(os.path.join(self.base_dir, 'logs', 'crawler.log'), encoding='utf-8')
        file_handler.setFormatter(logging.Formatter(log_format))
        logger.addHandler(file_handler)

        # Handler pour la console (avec couleurs)
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        # Marqueur de d√©but
        logging.info(f"D√©marrage du crawler avec le pattern de langue : {self.language_pattern}")

    def should_exclude(self, url):
        """D√©termine si une URL doit √™tre exclue en fonction des segments d√©finis"""
        for excluded in self.excluded_paths:
            if excluded in url:
                return True
        return False

    def is_same_language(self, url):
        """V√©rifie si l'URL respecte le m√™me pattern linguistique que l'URL de d√©part"""
        if not self.language_pattern:
            return True
        return self.language_pattern in url

    def is_downloadable_file(self, url):
        """V√©rifie si l'URL pointe vers un fichier t√©l√©chargeable en utilisant des expressions r√©guli√®res."""
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        # Cr√©er un pattern regex pour d√©tecter les extensions, m√™me avec des suffixes comme .pdf.aspx
        pattern = re.compile(r'\.(' + '|'.join([ext.strip('.') for exts in self.downloadable_extensions.values() for ext in exts]) + r')(\.[a-z0-9]+)?$', re.IGNORECASE)
        return bool(pattern.search(path))

    def get_file_type_and_extension(self, url, response):
        """
        D√©termine le type de fichier et l'extension appropri√©e en fonction de l'URL et du Content-Type.
        Retourne un tuple (file_type, extension).
        """
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()

        # Premi√®re tentative : d√©duire le type de fichier bas√© sur l'URL
        for file_type, extensions in self.downloadable_extensions.items():
            for ext in extensions:
                # Adapter le pattern pour inclure des suffixes comme .aspx
                pattern = re.compile(re.escape(ext) + r'(\.[a-z0-9]+)?$', re.IGNORECASE)
                if pattern.search(path):
                    return file_type, self.content_type_mapping[file_type].get(response.headers.get('Content-Type', '').lower(), ext)

        # Seconde tentative : d√©duire le type de fichier bas√© sur le Content-Type
        content_type = response.headers.get('Content-Type', '').lower()
        for file_type, mapping in self.content_type_mapping.items():
            if content_type in mapping:
                return file_type, mapping[content_type]

        # Si aucun type n'est d√©termin√©, retourner None
        return None, None

    def sanitize_filename(self, url, file_type, extension, page_number=None):
        """Cr√©e un nom de fichier s√©curis√© √† partir de l'URL, en ajustant l'extension si n√©cessaire."""
        # Cr√©ation d'un hash court de l'URL
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]

        # Extraction du dernier segment de l'URL
        filename = url.split('/')[-1]
        if not filename:
            filename = 'index'

        # Nettoyage du nom de fichier
        filename = re.sub(r'[^\w\-_.]', '_', filename)

        # Suppression des extensions existantes
        name, _ = os.path.splitext(filename)

        # D√©finir l'extension en fonction du type de fichier
        if not extension:
            extension = '.txt'  # Extension par d√©faut si non d√©termin√©e

        if page_number is not None:
            sanitized = f"{name}_page_{page_number:03d}_{url_hash}{extension}"
        else:
            sanitized = f"{name}_{url_hash}{extension}"

        logging.debug(f"Nom de fichier sanitiz√©: {sanitized}")
        return sanitized

    def download_file(self, url, file_type):
        """T√©l√©charge un fichier et le sauvegarde dans le dossier appropri√©."""
        try:
            logging.info(f"Tentative de t√©l√©chargement de fichier {file_type} depuis : {url}")

            # D√©terminer le type de fichier et l'extension
            response = self.session.head(url, allow_redirects=True, timeout=10)
            file_type_detected, extension = self.get_file_type_and_extension(url, response)
            if not file_type_detected:
                logging.warning(f"‚ö† Impossible de d√©terminer le type de fichier pour : {url}")
                return False

            # Renommer le fichier correctement
            filename = self.sanitize_filename(url, file_type_detected, extension)
            save_path = os.path.join(self.base_dir, file_type_detected, filename)

            # V√©rifier si le fichier existe d√©j√†
            if os.path.exists(save_path):
                logging.info(f"üìÇ Fichier d√©j√† t√©l√©charg√©, passage : {filename}")
                return False

            # T√©l√©charger le fichier avec une barre de progression
            response = self.session.get(url, stream=True, timeout=20)
            total_size = int(response.headers.get('content-length', 0))
            block_size = 1024  # 1 Kibibyte
            progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True, desc=f"‚è¨ Downloading {filename}", leave=False)

            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=block_size):
                    if chunk:
                        f.write(chunk)
                        progress_bar.update(len(chunk))
            progress_bar.close()

            if total_size != 0 and progress_bar.n != total_size:
                logging.warning(f"‚ö† T√©l√©chargement incomplet pour {url}")
                return False

            self.stats[f'{file_type_detected}_downloaded'] += 1
            self.downloaded_files.add(url)
            logging.info(f"‚úÖ T√©l√©chargement r√©ussi de {file_type_detected} : {filename}")
            return True

        except Exception as e:
            logging.error(f"‚úò Erreur lors du t√©l√©chargement de {url} : {str(e)}")
            return False

    def convert_links_to_absolute(self, soup, base_url):
        """Convertit tous les liens relatifs en liens absolus."""
        # Inclure les balises <embed>, <iframe>, et <object> en plus des <a> et <link>
        for tag in soup.find_all(['a', 'embed', 'iframe', 'object'], href=True):
            href = tag.get('href') or tag.get('src')
            if href:
                absolute_url = urljoin(base_url, href)
                if tag.name in ['embed', 'iframe', 'object']:
                    tag['src'] = absolute_url
                else:
                    tag['href'] = absolute_url
        return soup

    def clean_text(self, text):
        """Nettoie et formate le texte en r√©duisant les espaces inutiles tout en conservant une s√©paration claire."""
        if not text:
            return ""

        # Suppression des caract√®res sp√©ciaux inutiles
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)

        # Normalisation des espaces (remplacer les espaces multiples par un seul espace)
        text = re.sub(r'[ \t]+', ' ', text)

        # Normalisation des nouvelles lignes (remplacer les sauts de ligne multiples par deux sauts de ligne)
        text = re.sub(r'\n\s*\n', '\n\n', text)

        return text.strip()

    def fetch_page_content(self, url):
        """R√©cup√®re le contenu d'une page en utilisant Playwright ou requests."""
        if self.use_playwright:
            try:
                logging.info(f"üîç R√©cup√©ration avec Playwright : {url}")
                self.page.goto(url, timeout=20000)
                time.sleep(2)  # Attendre que la page soit compl√®tement charg√©e
                content = self.page.content()
                return content
            except Exception as e:
                logging.error(f"‚úò Playwright a √©chou√© √† r√©cup√©rer {url} : {str(e)}")
                return None
        else:
            try:
                response = self.session.get(url, timeout=20)
                if response.status_code == 200:
                    logging.info(f"‚úÖ Contenu r√©cup√©r√© avec succ√®s : {url}")
                    return response.text
                else:
                    logging.warning(f"‚ö† √âchec de r√©cup√©ration de {url} avec le code de statut {response.status_code}")
                    return None
            except Exception as e:
                logging.error(f"‚úò Requests a √©chou√© √† r√©cup√©rer {url} : {str(e)}")
                return None

    def extract_content(self, url):
        """Extrait le contenu d'une page et le sauvegarde en format markdown."""
        logging.info(f"üìÑ Extraction du contenu de : {url}")

        try:
            if self.is_downloadable_file(url):
                logging.info(f"üîó Passage de l'extraction du contenu pour le fichier t√©l√©chargeable : {url}")
                return

            page_content = self.fetch_page_content(url)
            if page_content is None:
                logging.warning(f"‚ö† Impossible de r√©cup√©rer le contenu pour : {url}")
                return

            soup = BeautifulSoup(page_content, 'html.parser')

            # Suppression des √©l√©ments non d√©sir√©s
            for element in soup.find_all(['nav', 'header', 'footer', 'script', 'style', 'aside', 'iframe']):
                element.decompose()

            # Extraction du contenu principal
            main_content = (
                soup.find('main') or
                soup.find('article') or
                soup.find('div', class_='content') or
                soup.find('div', id='content')
            )

            if main_content:
                # Conversion des liens relatifs en liens absolus
                main_content = self.convert_links_to_absolute(main_content, url)

                # Conversion du contenu principal en markdown
                markdown_content = self.html_converter.handle(str(main_content))

                # Construction du contenu final avec titre
                content_parts = []

                # Ajout du titre
                title = soup.find('h1')
                if title:
                    content_parts.append(f"# {title.get_text().strip()}")

                # Ajout de l'URL source
                content_parts.append(f"**Source:** {url}")

                # Ajout du contenu principal
                content_parts.append(markdown_content)

                # Nettoyage du contenu
                content = self.clean_text('\n\n'.join(content_parts))

                if content:
                    # Cr√©ation du nom de fichier
                    filename = self.sanitize_filename(url, 'Doc', '.txt')  # Utiliser 'Doc' avec extension '.txt' pour les pages
                    save_path = os.path.join(self.base_dir, 'content', filename)
                    with open(save_path, 'w', encoding='utf-8') as f:
                        f.write(content)

                    self.stats['pages_processed'] += 1
                    logging.info(f"‚úÖ Contenu sauvegard√© avec succ√®s dans : {filename}")
                else:
                    logging.warning(f"‚ö† Aucun contenu significatif trouv√© pour : {url}")

                # Traitement des fichiers t√©l√©chargeables trouv√©s dans la page
                downloadable_tags = main_content.find_all(['a', 'embed', 'iframe', 'object'], href=True)

                if downloadable_tags:
                    logging.info(f"üîÑ D√©tection de {len(downloadable_tags)} fichiers t√©l√©chargeables dans la page.")

                for tag in downloadable_tags:
                    href = tag.get('href') or tag.get('src')
                    if href:
                        file_url = urljoin(url, href)
                        if self.is_downloadable_file(file_url) and file_url not in self.downloaded_files:
                            # Utiliser HEAD pour obtenir le Content-Type sans t√©l√©charger le fichier
                            try:
                                response_head = self.session.head(file_url, allow_redirects=True, timeout=10)
                                file_type_detected, _ = self.get_file_type_and_extension(file_url, response_head)
                            except:
                                # Fallback √† GET si HEAD √©choue
                                response_head = self.session.get(file_url, allow_redirects=True, timeout=10)
                                file_type_detected, _ = self.get_file_type_and_extension(file_url, response_head)

                            if file_type_detected:
                                # Renommer le fichier correctement
                                filename = self.sanitize_filename(file_url, file_type_detected, self.content_type_mapping[file_type_detected].get(response_head.headers.get('Content-Type', '').lower(), ''))
                                save_path = os.path.join(self.base_dir, file_type_detected, filename)

                                if os.path.exists(save_path):
                                    logging.info(f"üìÇ Fichier d√©j√† t√©l√©charg√©, passage : {filename}")
                                    continue

                                self.download_file(file_url, file_type_detected)

            else:
                logging.warning(f"‚ö† Aucun contenu principal trouv√© pour : {url}")

        except Exception as e:
            logging.error(f"‚úò Erreur lors du traitement de {url} : {str(e)}")

    def extract_urls(self, start_url):
        """Extrait r√©cursivement les URLs d'une page jusqu'√† la profondeur maximale."""
        queue = deque()
        queue.append((start_url, 0))
        self.visited_pages.add(start_url)

        pbar = tqdm(total=1, desc="üîç Extraction des URLs", unit="page", ncols=100)

        while queue:
            current_url, depth = queue.popleft()

            if depth > self.max_depth:
                pbar.update(1)
                continue

            # V√©rifier si l'URL doit √™tre exclue
            if self.should_exclude(current_url):
                logging.info(f"üö´ URL exclue en raison d'un segment exclu : {current_url}")
                pbar.update(1)
                continue

            logging.info(f"üîé Extraction des URLs depuis : {current_url} (profondeur : {depth})")

            try:
                if self.is_downloadable_file(current_url):
                    # Utiliser HEAD pour obtenir le Content-Type sans t√©l√©charger le fichier
                    try:
                        response_head = self.session.head(current_url, allow_redirects=True, timeout=10)
                        file_type_detected, _ = self.get_file_type_and_extension(current_url, response_head)
                    except:
                        # Fallback √† GET si HEAD √©choue
                        response_head = self.session.get(current_url, allow_redirects=True, timeout=10)
                        file_type_detected, _ = self.get_file_type_and_extension(current_url, response_head)

                    if file_type_detected:
                        # Renommer le fichier pour v√©rifier l'existence
                        filename = self.sanitize_filename(current_url, file_type_detected, self.content_type_mapping[file_type_detected].get(response_head.headers.get('Content-Type', '').lower(), ''))
                        save_path = os.path.join(self.base_dir, file_type_detected, filename)

                        if os.path.exists(save_path):
                            logging.info(f"üìÇ Fichier d√©j√† t√©l√©charg√©, passage : {filename}")
                            pbar.update(1)
                            continue

                        self.download_file(current_url, file_type_detected)
                        self.downloaded_files.add(current_url)
                    pbar.update(1)
                    continue

                # R√©cup√©rer le contenu de la page (avec Playwright ou requests)
                page_content = self.fetch_page_content(current_url)
                if page_content is None:
                    logging.warning(f"‚ö† Impossible de r√©cup√©rer le contenu pour : {current_url}")
                    pbar.update(1)
                    continue

                soup = BeautifulSoup(page_content, 'html.parser')

                # Recherche de fichiers t√©l√©chargeables et de liens
                for tag in soup.find_all(['a', 'link', 'embed', 'iframe', 'object'], href=True):
                    href = tag.get('href') or tag.get('src')
                    if href:
                        absolute_url = urljoin(current_url, href)
                        parsed_url = urlparse(absolute_url)

                        # V√©rification si c'est un fichier t√©l√©chargeable
                        if self.is_downloadable_file(absolute_url):
                            # Utiliser HEAD pour obtenir le Content-Type sans t√©l√©charger le fichier
                            try:
                                response_head = self.session.head(absolute_url, allow_redirects=True, timeout=10)
                                file_type_detected, _ = self.get_file_type_and_extension(absolute_url, response_head)
                            except:
                                # Fallback √† GET si HEAD √©choue
                                response_head = self.session.get(absolute_url, allow_redirects=True, timeout=10)
                                file_type_detected, _ = self.get_file_type_and_extension(absolute_url, response_head)

                            if file_type_detected:
                                # Renommer le fichier pour v√©rifier l'existence
                                filename = self.sanitize_filename(absolute_url, file_type_detected, self.content_type_mapping[file_type_detected].get(response_head.headers.get('Content-Type', '').lower(), ''))
                                save_path = os.path.join(self.base_dir, file_type_detected, filename)

                                if os.path.exists(save_path):
                                    logging.info(f"üìÇ Fichier d√©j√† t√©l√©charg√©, passage : {filename}")
                                    continue

                                self.download_file(absolute_url, file_type_detected)
                                self.downloaded_files.add(absolute_url)
                            continue

                        # V√©rification des liens internes
                        if (self.domain in parsed_url.netloc and
                            self.is_same_language(absolute_url) and
                            absolute_url not in self.visited_pages and
                            not absolute_url.endswith(('#', 'javascript:void(0)', 'javascript:;')) and
                            not self.should_exclude(absolute_url)):

                            # Ajouter √† la queue avec profondeur incr√©ment√©e
                            queue.append((absolute_url, depth + 1))
                            self.visited_pages.add(absolute_url)
                            pbar.total += 1  # Augmenter la barre de progression
                            pbar.refresh()

            except Exception as e:
                logging.error(f"‚úò Erreur lors du crawling de {current_url} : {str(e)}")

            pbar.update(1)

        pbar.close()
        logging.info("üîç Extraction des URLs termin√©e.")

    def crawl(self):
        """M√©thode principale de crawling."""
        start_time = time.time()
        logging.info(f"üöÄ D√©but du crawl de {self.start_url}")
        logging.info(f"üåê Pattern de langue : {self.language_pattern}")
        logging.info(f"üìè Profondeur maximale : {self.max_depth}")

        # Charger les fichiers t√©l√©charg√©s pr√©c√©demment
        self.load_downloaded_files()

        # D√©marrer l'indicateur dynamique
        self.indicator.start()

        try:
            # Phase 1: Extraction des URLs
            logging.info("üîç Phase 1 : D√©but de l'extraction des URLs")
            self.extract_urls(self.start_url)

            # Phase 2: Extraction du contenu
            logging.info("üìÑ Phase 2 : D√©but de l'extraction du contenu")
            visited_without_files = [url for url in self.visited_pages if not self.is_downloadable_file(url)]
            
            pbar_content = tqdm(total=len(visited_without_files), desc="üìÑ Extraction du Contenu", unit="page", ncols=100)
            for i, url in enumerate(visited_without_files, 1):
                logging.info(f"üìù Traitement de l'URL {i}/{len(visited_without_files)} : {url}")
                self.extract_content(url)
                pbar_content.update(1)
            pbar_content.close()
            logging.info("üìÑ Extraction du contenu termin√©e.")

            # G√©n√©ration du rapport final
            end_time = time.time()
            self.generate_report(end_time - start_time)

        except Exception as e:
            logging.error(f"‚ö† Erreur critique durant le crawling : {str(e)}")
            self.generate_report(time.time() - start_time, error=str(e))

        finally:
            # Arr√™ter l'indicateur dynamique
            self.indicator.stop()

            # Sauvegarder les fichiers t√©l√©charg√©s
            self.save_downloaded_files()

            # Fermer Playwright si utilis√©
            if self.use_playwright:
                self.page.close()
                self.browser.close()
                self.playwright.stop()

    def load_downloaded_files(self):
        """Charge les URLs des fichiers d√©j√† t√©l√©charg√©s depuis le fichier de suivi."""
        downloaded_files_path = os.path.join(self.base_dir, 'logs', 'downloaded_files.txt')
        if os.path.exists(downloaded_files_path):
            with open(downloaded_files_path, 'r', encoding='utf-8') as f:
                for line in f:
                    self.downloaded_files.add(line.strip())
            logging.info(f"üì• Charg√© {len(self.downloaded_files)} fichiers t√©l√©charg√©s depuis le fichier de suivi.")
        else:
            logging.info("üÜï Aucun fichier de suivi des t√©l√©chargements trouv√©, d√©marrage √† z√©ro.")

    def save_downloaded_files(self):
        """Sauvegarde les URLs des fichiers t√©l√©charg√©s dans le fichier de suivi."""
        downloaded_files_path = os.path.join(self.base_dir, 'logs', 'downloaded_files.txt')
        try:
            with open(downloaded_files_path, 'w', encoding='utf-8') as f:
                for url in sorted(self.downloaded_files):
                    f.write(url + '\n')
            logging.info(f"üíæ Sauvegard√© {len(self.downloaded_files)} fichiers t√©l√©charg√©s dans le fichier de suivi.")
        except Exception as e:
            logging.error(f"‚úò Erreur lors de la sauvegarde du suivi des t√©l√©chargements : {str(e)}")

    def generate_report(self, duration, error=None):
        """G√©n√®re un rapport d√©taill√© du processus de crawling."""
        report_sections = []

        # En-t√™te du rapport
        report_sections.append(f"""  
Crawler Report  
==============  
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Configuration  
------------  
Start URL: {self.start_url}  
Language Pattern: {self.language_pattern}  
Max Depth: {self.max_depth}  
Duration: {duration:.2f} seconds

Statistics  
---------  
Total URLs found: {len(self.visited_pages)}  
Pages processed: {self.stats['pages_processed']}  
Files downloaded:  
- PDFs: {self.stats['PDF_downloaded']}  
- Images: {self.stats['Image_downloaded']}  
- Documents: {self.stats['Doc_downloaded']}  
""")

        # Section des erreurs si pr√©sentes
        if error:
            report_sections.append(f"""  
Errors  
------  
Critical Error: {error}

""")

        # Liste des URLs crawl√©es
        report_sections.append("""  
Processed URLs  
-------------  
""")
        for url in sorted(self.visited_pages):
            report_sections.append(url)

        # Liste des fichiers g√©n√©r√©s
        report_sections.append("""  
Generated Files  
--------------  
""")
        for directory in ['content', 'PDF', 'Image', 'Doc']:
            dir_path = os.path.join(self.base_dir, directory)
            if os.path.exists(dir_path):
                files = os.listdir(dir_path)
                report_sections.append(f"\n{directory} Files ({len(files)}):")
                for file in sorted(files):
                    report_sections.append(f"- {file}")

        # Sauvegarde du rapport
        report_content = '\n'.join(report_sections)
        report_path = os.path.join(self.base_dir, 'crawler_report.txt')

        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logging.info(f"üìÑ Rapport g√©n√©r√© avec succ√®s : {report_path}")
        except Exception as e:
            logging.error(f"‚úò Erreur lors de la g√©n√©ration du rapport : {str(e)}")

        # Cr√©ation d'un fichier de r√©sum√©
        summary = f"""  
Crawling Summary  
---------------  
Start URL: {self.start_url}  
Total URLs: {len(self.visited_pages)}  
Pages Processed: {self.stats['pages_processed']}  
Total Files Downloaded: {sum(self.stats[k] for k in ['PDF_downloaded', 'Image_downloaded', 'Doc_downloaded'])}  
Duration: {duration:.2f} seconds  
Status: {'‚ö† Completed with errors' if error else '‚úÖ Completed successfully'}  
"""
        try:
            with open(os.path.join(self.base_dir, 'summary.txt'), 'w', encoding='utf-8') as f:
                f.write(summary)
            logging.info(f"üìÑ R√©sum√© g√©n√©r√© avec succ√®s : {os.path.join(self.base_dir, 'summary.txt')}")
        except Exception as e:
            logging.error(f"‚úò Erreur lors de la g√©n√©ration du r√©sum√© : {str(e)}")

# Classe pour l'Extraction PDF/DOC
class PDFExtractor:
    def __init__(self, input_dir, output_dir, api_keys_file, max_workers=10, initial_dpi=300, retry_dpi=200, logger=None):
        # Configuration des chemins
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Chargement des cl√©s API OpenAI depuis un fichier
        self.api_keys = Queue()
        try:
            with open(api_keys_file, 'r') as f:
                for line in f:
                    key = line.strip()
                    if key:
                        self.api_keys.put(key)
            if self.api_keys.empty():
                raise ValueError("Aucune cl√© API charg√©e.")
        except Exception as e:
            if logger:
                logger.error(f"üö´ Erreur lors du chargement des cl√©s API: {str(e)}")
            else:
                print(f"üö´ Erreur lors du chargement des cl√©s API: {str(e)}")
            raise
        
        self.max_workers = max_workers
        self.initial_dpi = initial_dpi
        self.retry_dpi = retry_dpi
        self.logger = logger or logging.getLogger(__name__)

    def preprocess_image(self, image):
        """Pr√©traitement de l'image pour OCR"""
        if isinstance(image, Image.Image):
            image = np.array(image)

        if image is None:
            raise ValueError("‚ö†Ô∏è Image vide ou corrompue.")
        
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image

            denoised = cv2.fastNlMeansDenoising(gray)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(denoised)
            binary = cv2.adaptiveThreshold(
                enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                cv2.THRESH_BINARY, 11, 2
            )
            return binary
        except Exception as e:
            raise ValueError(f"‚ö†Ô∏è Erreur lors du pr√©traitement de l'image: {str(e)}")

    def convert_pdf_to_images(self, pdf_path, dpi):
        """Convertit un PDF en images avec un DPI sp√©cifique"""
        try:
            self.logger.info(f"üìÑ Conversion de {pdf_path.name} en images avec DPI={dpi}")
            images = convert_from_path(pdf_path, dpi=dpi, fmt='jpeg', thread_count=1)
            self.logger.info(f"‚úÖ Conversion r√©ussie pour {pdf_path.name} avec DPI={dpi}")
            return images
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la conversion de {pdf_path.name} en images avec DPI={dpi}: {str(e)}")
            return None

    def extract_text_with_ocr(self, pdf_path):
        """Extraction de texte par OCR avec gestion des erreurs et reprises"""
        # Premi√®re tentative avec le DPI initial
        images = self.convert_pdf_to_images(pdf_path, self.initial_dpi)
        if images is None:
            # Deuxi√®me tentative avec un DPI r√©duit
            images = self.convert_pdf_to_images(pdf_path, self.retry_dpi)
            if images is None:
                self.logger.error(f"‚ùå √âchec de la conversion de {pdf_path.name} en images avec tous les DPI tent√©s.")
                return None
        
        ocr_texts = []
        for i, image in enumerate(images, 1):
            self.logger.info(f"üîç OCR page {i}/{len(images)} de {pdf_path.name}")
            try:
                processed_img = self.preprocess_image(image)
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è Pr√©traitement de l'image √©chou√© pour la page {i} de {pdf_path.name}: {str(e)}")
                ocr_texts.append("")
                continue

            try:
                text = pytesseract.image_to_string(
                    processed_img,
                    lang='fra+eng',
                    config='--psm 1'
                )
                if len(text.strip()) < 100:
                    self.logger.info(f"üîÑ OCR insuffisant (moins de 100 caract√®res) pour la page {i} de {pdf_path.name}, tentative avec psm 3 et oem 1")
                    text = pytesseract.image_to_string(
                        processed_img,
                        lang='fra+eng',
                        config='--psm 3 --oem 1'
                    )
                ocr_texts.append(text)
            except Exception as e:
                self.logger.error(f"‚ùå OCR √©chou√© pour la page {i} de {pdf_path.name}: {str(e)}")
                ocr_texts.append("")
        
        return ocr_texts

    def extract_text_with_pypdf_per_page(self, pdf_path, page_num):
        """Extraction texte avec PyPDF pour une page sp√©cifique"""
        try:
            with open(pdf_path, 'rb') as file:
                reader = pypdf.PdfReader(file)
                if page_num < 1 or page_num > len(reader.pages):
                    self.logger.error(f"‚ö†Ô∏è Num√©ro de page invalide: {page_num} dans {pdf_path.name}")
                    return ''
                page = reader.pages[page_num - 1]
                text = page.extract_text() or ''
                self.logger.info(f"üìù Extraction PyPDF page {page_num} de {pdf_path.name}: {len(text)} caract√®res")
                return text
        except Exception as e:
            self.logger.error(f"‚ùå Erreur PyPDF pour la page {page_num} de {pdf_path.name}: {str(e)}")
            return ''

    def get_api_key(self):
        """R√©cup√®re une cl√© API de la queue"""
        try:
            api_key = self.api_keys.get(timeout=10)
            return api_key
        except Empty:
            self.logger.error("üö´ Aucune cl√© API disponible.")
            return None

    def process_with_gpt(self, content):
        """Traitement du contenu avec GPT-4 pour structurer le texte en Markdown"""
        system_prompt = {
            "role": "system",
            "content": (
                "Vous √™tes un assistant expert en analyse de documents. Votre t√¢che est de : "
                "1. Extraire et structurer les informations cl√©s du texte fourni en suivant ces r√®gles :\n"
                "   - Cr√©er une hi√©rarchie claire avec des titres (# ## ###)\n"
                "   - S√©parer les sections par des sauts de ligne\n"
                "   - Assurer une coh√©rence dans la pr√©sentation\n\n"
                "2. Pour les tableaux :\n"
                "   - Convertir chaque ligne en items de liste\n"
                "   - Utiliser le format '- **[Nom colonne] :** [valeur]'\n"
                "   - Grouper les items li√©s avec une indentation\n"
                "   - Ajouter des s√©parateurs '---' entre les groupes\n\n"
                "3. Appliquer le formatage suivant :\n"
                "   - Mettre en italique (*) les termes importants\n"
                "   - Utiliser le gras (**) pour les titres de colonnes\n"
                "   - Cr√©er des listes √† puces (-) pour les √©num√©rations\n"
                "   - Utiliser des citations (>) pour les notes importantes\n\n"
                "4. Nettoyer et am√©liorer le texte :\n"
                "   - Corriger les erreurs typographiques d'OCR\n"
                "   - Unifier la ponctuation\n"
                "   - √âliminer les caract√®res parasites\n"
                "   - V√©rifier l'alignement et l'espacement\n\n"
                "Exemple de transformation :\n"
                "Table : 'Produit | Prix | Stock\n"
                "         Pommes | 2.50 | 100\n"
                "         Poires | 3.00 | 85'\n\n"
                "Devient :\n"
                "### Liste des produits\n\n"
                "- **Produit :** Pommes\n"
                "  - **Prix :** 2.50‚Ç¨\n"
                "  - **Stock :** 100 unit√©s\n\n"
                "---\n\n"
                "- **Produit :** Poires\n"
                "  - **Prix :** 3.00‚Ç¨\n"
                "  - **Stock :** 85 unit√©s"
            )
        }

        api_key = self.get_api_key()
        if not api_key:
            return None

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": "gpt-4",
            "messages": [
                system_prompt,
                {"role": "user", "content": content}
            ],
            "temperature": 0,
            "max_tokens": 16000,
            "top_p": 1,
            "frequency_penalty": 0,
            "presence_penalty": 0
        }

        try:
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            processed_content = response.json()['choices'][0]['message']['content']
            time.sleep(1)  # Pause pour respecter les limites de taux
            return processed_content
        except Exception as e:
            self.logger.error(f"‚ùå Erreur GPT: {str(e)}")
            return None
        finally:
            # Remettre la cl√© API dans la queue
            if api_key:
                self.api_keys.put(api_key)

    def split_content(self, content, max_length=4000):
        """Divise le contenu en parties plus petites si n√©cessaire"""
        paragraphs = content.split('\n\n')
        parts = []
        current_part = ""
        for para in paragraphs:
            if len(current_part) + len(para) + 2 > max_length:
                if current_part:
                    parts.append(current_part.strip())
                current_part = para + '\n\n'
            else:
                current_part += para + '\n\n'
        if current_part.strip():
            parts.append(current_part.strip())
        return parts

    def process_single_part(self, document_name, page_num, part_num, content):
        """Traitement d'une seule partie du contenu avec GPT"""
        self.logger.info(f"üìù Traitement du Document: {document_name}, Page: {page_num}, Partie: {part_num}")
        processed_content = self.process_with_gpt(content)
        
        if processed_content:
            output_file_name = self.output_dir / f"{document_name}_page_{page_num}_part_{part_num}.txt"
            try:
                with open(output_file_name, 'a', encoding='utf-8') as f:
                    f.write(f"üìÑ Document ID: {document_name}\n\n{processed_content}\n\n")
                self.logger.info(f"‚úÖ Fichier cr√©√©: {output_file_name.name}")
            except Exception as e:
                self.logger.error(f"‚ùå Erreur sauvegarde Document: {document_name}, Page: {page_num}, Partie: {part_num}: {str(e)}")

    def process_pdf(self, pdf_path):
        """Traitement complet d'un PDF"""
        document_name = pdf_path.stem
        self.logger.info(f"üìÇ D√©but traitement de {pdf_path.name}")
        
        # Extraction de texte par OCR
        ocr_texts = self.extract_text_with_ocr(pdf_path)
        if ocr_texts is None:
            self.logger.error(f"‚ùå √âchec de l'extraction OCR pour {pdf_path.name}")
            return False

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for page_num, ocr_text in enumerate(ocr_texts, 1):
                self.logger.info(f"üîÑ Pr√©paration de la page {page_num}/{len(ocr_texts)} de {pdf_path.name}")
                
                if ocr_text and len(ocr_text.strip()) >= 100:
                    page_text = ocr_text
                    self.logger.info(f"‚úÖ OCR r√©ussi pour la page {page_num} de {pdf_path.name}")
                else:
                    self.logger.info(f"üîÑ OCR insuffisant pour la page {page_num} de {pdf_path.name}, utilisation de PyPDF")
                    pypdf_text = self.extract_text_with_pypdf_per_page(pdf_path, page_num)
                    page_text = pypdf_text
                
                if not page_text.strip():
                    self.logger.warning(f"‚ö†Ô∏è Aucun texte extrait pour la page {page_num} de {pdf_path.name}")
                    continue  # Passer √† la page suivante si aucun texte n'est extrait
                
                # Diviser le texte si trop long
                parts = self.split_content(page_text, max_length=4000)  # Ajustez max_length si n√©cessaire
                
                for idx, part in enumerate(parts, 1):
                    futures.append(executor.submit(
                        self.process_single_part, document_name, page_num, idx, part
                    ))
            
            # Attente des t√¢ches termin√©es avec mise √† jour de la barre de progression
            for future in as_completed(futures):
                pass  # Les logs sont g√©r√©s dans les t√¢ches individuelles
        
        self.logger.info(f"‚úÖ Termin√© traitement de {pdf_path.name}")
        return True

    def process_all_pdfs(self):
        """Traitement de tous les PDF"""
        pdf_files = list(self.input_dir.glob('*.pdf'))
        total_files = len(pdf_files)
        
        self.logger.info(f"üì¢ D√©but traitement de {total_files} fichiers PDF dans '{self.input_dir}'")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=Console()
        ) as progress:
            pdf_progress = progress.add_task("[green]Traitement des PDF...", total=total_files)
            for pdf_path in pdf_files:
                self.process_pdf(pdf_path)
                progress.update(pdf_progress, advance=1)
        
        self.logger.info(f"üéâ Termin√©. {total_files} fichiers PDF trait√©s.")

# Classe pour le Traitement des Embeddings
class EmbeddingProcessor:
    def __init__(self, input_dir, output_dir, openai_api_keys, verbose=False, logger=None):
        # Configuration des chemins
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialisation des listes globales pour tous les fichiers
        self.all_embeddings = []
        self.all_metadata = []

        # Configuration OpenAI
        self.openai_api_keys = openai_api_keys
        self.headers_cycle = cycle([
            {
                "Authorization": f"Bearer {key}",
                "Content-Type": "application/json"
            } for key in self.openai_api_keys
        ])
        self.lock = threading.Lock()

        # Configuration logging
        self.logger = logger or logging.getLogger(__name__)
        self.verbose = verbose

    def chunk_text(self, text, chunk_size=1200, overlap_size=100):
        """D√©coupe le texte en chunks avec un chevauchement. Traite aussi les textes courts."""
        try:
            tokens = text.split()
            
            # Si le texte est plus court que la taille du chunk, le traiter comme un seul chunk
            if len(tokens) <= chunk_size:
                return [text]
                
            chunks = []
            for i in range(0, len(tokens), chunk_size - overlap_size):
                chunk = ' '.join(tokens[i:i + chunk_size])
                chunks.append(chunk)
                
            # S'assurer que le dernier chunk n'est pas trop petit
            if len(chunks) > 1 and len(tokens[-(chunk_size - overlap_size):]) < chunk_size // 2:
                # Fusionner le dernier chunk avec l'avant-dernier s'il est trop petit
                last_chunk = ' '.join(tokens[-(chunk_size):])
                chunks[-1] = last_chunk
            
            return chunks
            
        except Exception as e:
            self.logger.error(f"Erreur lors du d√©coupage du texte: {str(e)}")
            return [text]  # Retourner le texte complet en cas d'erreur

    def get_contextualized_chunk(self, chunk, full_text, headers, document_name, page_num, chunk_id):
        """Demande √† GPT-4 de contextualiser chaque chunk."""
        try:
            system_prompt = {
                "role": "system",
                "content": (
                    "Vous √™tes un expert analyste. Le texte suivant est un extrait d'un document plus important. "
                    "Votre t√¢che consiste √† fournir un contexte √† la section suivante en faisant r√©f√©rence au contenu de l'ensemble du document. "
                    "Veiller √† ce que le contexte permette de mieux comprendre le morceau."
                )
            }
            user_prompt = {
                "role": "user",
                "content": f"Document: {full_text}\n\nChunk: {chunk}\n\nVeuillez fournir un contexte pour ce morceau en fran√ßais"
            }

            payload = {
                "model": "gpt-4",
                "messages": [system_prompt, user_prompt],
                "temperature": 0,
                "max_tokens": 200,
                "top_p": 1,
                "frequency_penalty": 0,
                "presence_penalty": 0
            }

            if self.verbose:
                self.logger.info(f"Appel LLM pour {document_name} page {page_num}, chunk {chunk_id}")

            self.logger.info(f"D√©but de l'appel API GPT pour {document_name} page {page_num} chunk {chunk_id}")  
            
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']

        except Exception as e:
            self.logger.error(f"Erreur lors de la contextualisation: {str(e)}")
            return None

    def get_embedding(self, text, headers, document_name, page_num, chunk_id):
        """Obtenir l'embedding pour un texte."""
        try:
            payload = {
                "input": text,
                "model": "text-embedding-ada-002",  # Utiliser le mod√®le d'embedding appropri√©
                "encoding_format": "float"
            }

            if self.verbose:
                self.logger.info(f"Appel API Embedding pour {document_name} page {page_num} chunk {chunk_id}")

            self.logger.info(f"üîó Appel API Embedding pour {document_name} page {page_num} chunk {chunk_id}")  
            
            response = requests.post(
                'https://api.openai.com/v1/embeddings',
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            return response.json()['data'][0]['embedding']

        except Exception as e:
            self.logger.error(f"Erreur lors de la r√©cup√©ration de l'embedding: {str(e)}")
            return None

    def process_chunk(self, chunk_info):
        """Processus pour un chunk sp√©cifique."""
        try:
            txt_file_path, chunk_id, chunk, full_text, document_name, page_num = chunk_info

            with self.lock:
                headers = next(self.headers_cycle)

            context = self.get_contextualized_chunk(chunk, full_text, headers, document_name, page_num, chunk_id)  
            if not context:
                return None, None

            combined_text = f"{context}\n\nContext:\n{chunk}"  
            embedding = self.get_embedding(combined_text, headers, document_name, page_num, chunk_id)  
            
            if embedding:
                metadata = {
                    "filename": txt_file_path.name,
                    "chunk_id": chunk_id,
                    "page_num": page_num,
                    "text_raw": chunk,
                    "context": context,
                    "text": combined_text
                }
                return embedding, metadata

            return None, None

        except Exception as e:
            self.logger.error(f"Erreur lors du traitement du chunk: {str(e)}")
            return None, None

    def process_file(self, txt_file_path):
        """Processus pour un fichier texte."""
        try:
            self.logger.info(f"üìÇ Traitement du fichier: {txt_file_path}")
            
            with open(txt_file_path, 'r', encoding='utf-8') as file:
                full_text = file.read()

            chunks = self.chunk_text(full_text)
            
            # Extraire le num√©ro de page du nom du fichier
            match = re.search(r'_page_(\d+)_', txt_file_path.stem)
            if match:
                page_num = int(match.group(1))
            else:
                page_num = 1  # Par d√©faut si non trouv√©
            
            chunk_infos = [
                (txt_file_path, i, chunk, full_text, txt_file_path.stem, page_num)
                for i, chunk in enumerate(chunks, 1)
            ]

            return chunk_infos

        except Exception as e:
            self.logger.error(f"Erreur lors du traitement du fichier {txt_file_path}: {str(e)}")
            return []

    def process_all_files(self):
        """Processus pour tous les fichiers dans le dossier d'entr√©e avec parall√©lisme."""
        try:
            txt_files = list(self.input_dir.glob('*.txt'))
            total_files = len(txt_files)
            self.logger.info(f"üì¢ D√©but traitement de {total_files} fichiers dans '{self.input_dir}'")

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                for txt_file_path in txt_files:
                    chunk_infos = self.process_file(txt_file_path)
                    for chunk_info in chunk_infos:
                        futures.append(executor.submit(self.process_chunk, chunk_info))

                for future in as_completed(futures):
                    embedding, metadata = future.result()
                    if embedding and metadata:
                        self.all_embeddings.append(embedding)
                        self.all_metadata.append(metadata)

            if self.all_embeddings:
                # Sauvegarde des r√©sultats
                chunks_json_path = self.output_dir / "chunks.json"
                with open(chunks_json_path, 'w', encoding='utf-8') as json_file:
                    json.dump({"metadata": self.all_metadata}, json_file, ensure_ascii=False, indent=4)

                embeddings_npy_path = self.output_dir / "embeddings.npy"
                np.save(embeddings_npy_path, np.array(self.all_embeddings))

                self.logger.info(f"‚úÖ Fichiers cr√©√©s: {chunks_json_path} et {embeddings_npy_path}")

        except Exception as e:
            self.logger.error(f"Erreur lors du traitement des fichiers: {str(e)}")
            raise

# Classe Principale pour l'Int√©gration
class IntegrationManager:
    def __init__(self, config):
        # Charger les param√®tres depuis le dictionnaire de configuration
        self.crawler_config = config.get('crawler', {})
        self.extractor_config = config.get('extractor', {})
        self.embedding_config = config.get('embedding', {})
        
        # Configurer le logging
        self.setup_logging(config.get('logging', {}))
        self.logger = logging.getLogger('IntegrationManager')
        
        # Initialiser les composants
        self.crawler = WebCrawler(
            start_url=self.crawler_config.get('start_url'),
            max_depth=self.crawler_config.get('max_depth', 2),
            use_playwright=self.crawler_config.get('use_playwright', False),
            excluded_paths=self.crawler_config.get('excluded_paths'),
            download_extensions=self.crawler_config.get('download_extensions'),
            language_pattern=self.crawler_config.get('language_pattern'),
            base_dir=self.crawler_config.get('base_dir')
        )
        
        self.extractor = PDFExtractor(
            input_dir=self.extractor_config.get('input_dir'),
            output_dir=self.extractor_config.get('output_dir'),
            api_keys_file=self.extractor_config.get('api_keys_file'),
            max_workers=self.extractor_config.get('max_workers', 10),
            initial_dpi=self.extractor_config.get('initial_dpi', 300),
            retry_dpi=self.extractor_config.get('retry_dpi', 200),
            logger=self.logger
        )
        
        self.embedding = EmbeddingProcessor(
            input_dir=self.embedding_config.get('input_dir'),
            output_dir=self.embedding_config.get('output_dir'),
            openai_api_keys=self.embedding_config.get('openai_api_keys'),
            verbose=self.embedding_config.get('verbose', False),
            logger=self.logger
        )
    
    def setup_logging(self, logging_config):
        log_level = logging_config.get('level', 'INFO').upper()
        log_format = logging_config.get('format', '%(asctime)s - %(levelname)s - %(message)s')
        log_file = logging_config.get('file', 'integration_manager.log')
        
        logging.basicConfig(
            level=log_level,
            format=log_format,
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    
    def run_crawling(self):
        self.logger.info("üõ†Ô∏è  D√©but du crawling")
        self.crawler.crawl()
        self.logger.info("‚úÖ Crawling termin√©")
    
    def run_extraction(self):
        self.logger.info("üõ†Ô∏è  D√©but de l'extraction PDF/DOC")
        self.extractor.process_all_pdfs()
        self.logger.info("‚úÖ Extraction PDF/DOC termin√©e")
    
    def run_embedding(self):
        self.logger.info("üõ†Ô∏è  D√©but du traitement des embeddings")
        self.embedding.process_all_files()
        self.logger.info("‚úÖ Traitement des embeddings termin√©")
    
    def run_all(self):
        try:
            self.run_crawling()
            self.run_extraction()
            self.run_embedding()
            self.logger.info("üéâ Processus complet termin√© avec succ√®s")
        except Exception as e:
            self.logger.error(f"‚ùå Erreur durant le processus int√©gr√©: {str(e)}")
            raise

# Fonction Principale
def main():
    # Charger les variables d'environnement si n√©cessaire
    load_dotenv()
    
    # D√©finir les param√®tres de configuration
    config = {
        'crawler': {
            'start_url': "https://liquid-air.ca",
            'max_depth': 3,
            'use_playwright': True,
            'excluded_paths': ['selecteur-de-produits'],
            'download_extensions': {
                'PDF': ['.pdf'],
                'Image': ['.png', '.jpg', '.jpeg', '.gif', '.svg'],
                'Doc': ['.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
            },
            'language_pattern': re.search(r'/(fr|en)-(ca|us)/', "https://liquid-air.ca"),  # Ajustez selon besoin
            'base_dir': "crawler_output"  # Optionnel, sinon g√©n√©r√© automatiquement
        },
        'extractor': {
            'input_dir': "crawler_output/PDF",
            'output_dir': "extraction_output/content",
            'api_keys_file': "api_keys.txt",
            'max_workers': 10,
            'initial_dpi': 300,
            'retry_dpi': 200
        },
        'embedding': {
            'input_dir': "extraction_output/content",
            'output_dir': "embeddings_output",
            'openai_api_keys': [key.strip() for key in open('api_keys.txt').readlines() if key.strip()],
            'verbose': False
        },
        'logging': {
            'level': 'INFO',
            'format': '%(asctime)s - %(levelname)s - %(message)s',
            'file': 'integration_manager.log'
        }
    }
    
    # V√©rifier les cl√©s API OpenAI
    if not config['embedding']['openai_api_keys']:
        print("üö´ Aucune cl√© API OpenAI trouv√©e. Veuillez v√©rifier le fichier 'api_keys.txt'.")
        return
    
    # Initialiser et ex√©cuter l'int√©gration
    try:
        manager = IntegrationManager(config)
        manager.run_all()
    except Exception as e:
        print(f"‚ùå Erreur lors de l'int√©gration: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
